{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Neural Network for Node Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "import math\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Convolution Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    def __init__(self, feature_numbers, out_numbers, bias=False) -> None:\n",
    "        super().__init__()\n",
    "        self.bias = bias\n",
    "        self.w = Parameter(torch.FloatTensor(feature_numbers, out_numbers))\n",
    "        if bias:\n",
    "            self.b = Parameter(torch.FloatTensor(out_numbers))\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.w.size(1))\n",
    "        self.w.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not False:\n",
    "            self.b.data.uniform_(-stdv, stdv)\n",
    "            \n",
    "    def forward(self, x, adj):\n",
    "        support = torch.mm(x, self.w)\n",
    "        out = torch.spmm(adj, support)\n",
    "        if self.bias:\n",
    "            out = out + self.b\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeClassificationGCNN(nn.Module):\n",
    "\n",
    "    def __init__(self, feature_num, node_representation_dim, nclass, droupout=0.2, bias=False) -> None:\n",
    "        super().__init__()\n",
    "        self.gconv1 = GraphConvolution(feature_num, node_representation_dim, bias)\n",
    "        self.gconv2 = GraphConvolution(node_representation_dim, nclass, bias)\n",
    "        self.dropout = droupout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gconv1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, self.training)\n",
    "        x = F.relu(self.gconv2(x, adj))\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_205002/3819785820.py:25: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:651.)\n",
      "  return torch.sparse.FloatTensor(indices, values, shape)\n"
     ]
    }
   ],
   "source": [
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "def load_data(path=\"../data/cora/\", dataset=\"cora\"):\n",
    "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
    "                                        dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n",
    "                                    dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "\n",
    "    # build symmetric adjacency matrix\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    features = normalize(features)\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test\n",
    "\n",
    "adj, features, labels, idx_train, idx_val, idx_test=load_data(path=\"../data/cora/\", dataset=\"cora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NodeClassificationGCNN(features.shape[1], 256, np.max(labels.detach().numpy())+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out,label):\n",
    "    oneHotCodded = out.max(1)[1].type_as(label)\n",
    "    return oneHotCodded.eq(label).double().sum()/len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0 ; accuracy: 0.17857142857142858; loss: 1.9436419010162354\n",
      "Validation epoch 0 ; accuracy: 0.6066666666666667; loss: 1.8875856399536133\n",
      "Training epoch 1 ; accuracy: 0.6571428571428571; loss: 1.8710966110229492\n",
      "Validation epoch 1 ; accuracy: 0.6233333333333333; loss: 1.81190025806427\n",
      "Training epoch 2 ; accuracy: 0.7; loss: 1.7783282995224\n",
      "Validation epoch 2 ; accuracy: 0.66; loss: 1.7257922887802124\n",
      "Training epoch 3 ; accuracy: 0.7571428571428571; loss: 1.667269229888916\n",
      "Validation epoch 3 ; accuracy: 0.6833333333333333; loss: 1.634028673171997\n",
      "Training epoch 4 ; accuracy: 0.8071428571428572; loss: 1.548073410987854\n",
      "Validation epoch 4 ; accuracy: 0.7033333333333334; loss: 1.538655161857605\n",
      "Training epoch 5 ; accuracy: 0.85; loss: 1.421162724494934\n",
      "Validation epoch 5 ; accuracy: 0.7266666666666667; loss: 1.443504810333252\n",
      "Training epoch 6 ; accuracy: 0.8571428571428571; loss: 1.2948830127716064\n",
      "Validation epoch 6 ; accuracy: 0.7366666666666667; loss: 1.3469977378845215\n",
      "Training epoch 7 ; accuracy: 0.8785714285714286; loss: 1.1673847436904907\n",
      "Validation epoch 7 ; accuracy: 0.7533333333333333; loss: 1.2522152662277222\n",
      "Training epoch 8 ; accuracy: 0.8785714285714286; loss: 1.044338345527649\n",
      "Validation epoch 8 ; accuracy: 0.76; loss: 1.161211609840393\n",
      "Training epoch 9 ; accuracy: 0.9; loss: 0.9288725852966309\n",
      "Validation epoch 9 ; accuracy: 0.7866666666666666; loss: 1.0748411417007446\n",
      "Training epoch 10 ; accuracy: 0.9; loss: 0.8225880861282349\n",
      "Validation epoch 10 ; accuracy: 0.8033333333333333; loss: 0.9945563673973083\n",
      "Training epoch 11 ; accuracy: 0.9142857142857143; loss: 0.7200027108192444\n",
      "Validation epoch 11 ; accuracy: 0.8166666666666667; loss: 0.9213541746139526\n",
      "Training epoch 12 ; accuracy: 0.9357142857142857; loss: 0.6231727600097656\n",
      "Validation epoch 12 ; accuracy: 0.8233333333333334; loss: 0.8558281660079956\n",
      "Training epoch 13 ; accuracy: 0.9428571428571428; loss: 0.5503323078155518\n",
      "Validation epoch 13 ; accuracy: 0.8266666666666667; loss: 0.798047661781311\n",
      "Training epoch 14 ; accuracy: 0.9571428571428572; loss: 0.47091472148895264\n",
      "Validation epoch 14 ; accuracy: 0.8333333333333334; loss: 0.7478172779083252\n",
      "Training epoch 15 ; accuracy: 0.9642857142857143; loss: 0.40580758452415466\n",
      "Validation epoch 15 ; accuracy: 0.84; loss: 0.7050489187240601\n",
      "Training epoch 16 ; accuracy: 0.9642857142857143; loss: 0.3529333770275116\n",
      "Validation epoch 16 ; accuracy: 0.84; loss: 0.6693391799926758\n",
      "Training epoch 17 ; accuracy: 0.9642857142857143; loss: 0.30938830971717834\n",
      "Validation epoch 17 ; accuracy: 0.84; loss: 0.6404998302459717\n",
      "Training epoch 18 ; accuracy: 0.9714285714285714; loss: 0.2687450647354126\n",
      "Validation epoch 18 ; accuracy: 0.8366666666666667; loss: 0.6181120872497559\n",
      "Training epoch 19 ; accuracy: 0.9714285714285714; loss: 0.22986149787902832\n",
      "Validation epoch 19 ; accuracy: 0.8266666666666667; loss: 0.6013576984405518\n",
      "Training epoch 20 ; accuracy: 0.9714285714285714; loss: 0.2041216790676117\n",
      "Validation epoch 20 ; accuracy: 0.8266666666666667; loss: 0.5897331237792969\n",
      "Training epoch 21 ; accuracy: 0.9714285714285714; loss: 0.17480210959911346\n",
      "Validation epoch 21 ; accuracy: 0.8266666666666667; loss: 0.5827645659446716\n",
      "Training epoch 22 ; accuracy: 0.9714285714285714; loss: 0.15515072643756866\n",
      "Validation epoch 22 ; accuracy: 0.8333333333333334; loss: 0.5795834064483643\n",
      "Training epoch 23 ; accuracy: 0.9714285714285714; loss: 0.13571693003177643\n",
      "Validation epoch 23 ; accuracy: 0.83; loss: 0.5790910124778748\n",
      "Training epoch 24 ; accuracy: 0.9785714285714285; loss: 0.1172347441315651\n",
      "Validation epoch 24 ; accuracy: 0.8333333333333334; loss: 0.5810549259185791\n",
      "Training epoch 25 ; accuracy: 0.9857142857142858; loss: 0.10573709011077881\n",
      "Validation epoch 25 ; accuracy: 0.8333333333333334; loss: 0.5848586559295654\n",
      "Training epoch 26 ; accuracy: 0.9857142857142858; loss: 0.09548574686050415\n",
      "Validation epoch 26 ; accuracy: 0.83; loss: 0.5899485349655151\n",
      "Training epoch 27 ; accuracy: 0.9857142857142858; loss: 0.08112041652202606\n",
      "Validation epoch 27 ; accuracy: 0.8266666666666667; loss: 0.5959373116493225\n",
      "Training epoch 28 ; accuracy: 0.9857142857142858; loss: 0.07210013270378113\n",
      "Validation epoch 28 ; accuracy: 0.83; loss: 0.6020704507827759\n",
      "Training epoch 29 ; accuracy: 0.9857142857142858; loss: 0.06386004388332367\n",
      "Validation epoch 29 ; accuracy: 0.83; loss: 0.6082244515419006\n",
      "Training epoch 30 ; accuracy: 0.9928571428571429; loss: 0.054470181465148926\n",
      "Validation epoch 30 ; accuracy: 0.8333333333333334; loss: 0.6143275499343872\n",
      "Training epoch 31 ; accuracy: 1.0; loss: 0.049863673746585846\n",
      "Validation epoch 31 ; accuracy: 0.8333333333333334; loss: 0.6204860210418701\n",
      "Training epoch 32 ; accuracy: 1.0; loss: 0.04440890997648239\n",
      "Validation epoch 32 ; accuracy: 0.8366666666666667; loss: 0.6262316107749939\n",
      "Training epoch 33 ; accuracy: 1.0; loss: 0.041391775012016296\n",
      "Validation epoch 33 ; accuracy: 0.8333333333333334; loss: 0.6315985321998596\n",
      "Training epoch 34 ; accuracy: 1.0; loss: 0.0365462452173233\n",
      "Validation epoch 34 ; accuracy: 0.8333333333333334; loss: 0.6372281312942505\n",
      "Training epoch 35 ; accuracy: 1.0; loss: 0.03220057114958763\n",
      "Validation epoch 35 ; accuracy: 0.8366666666666667; loss: 0.6430978178977966\n",
      "Training epoch 36 ; accuracy: 1.0; loss: 0.031982384622097015\n",
      "Validation epoch 36 ; accuracy: 0.8366666666666667; loss: 0.6482372879981995\n",
      "Training epoch 37 ; accuracy: 1.0; loss: 0.027027353644371033\n",
      "Validation epoch 37 ; accuracy: 0.8366666666666667; loss: 0.6531231999397278\n",
      "Training epoch 38 ; accuracy: 1.0; loss: 0.02352404221892357\n",
      "Validation epoch 38 ; accuracy: 0.8366666666666667; loss: 0.6580184102058411\n",
      "Training epoch 39 ; accuracy: 1.0; loss: 0.023005004972219467\n",
      "Validation epoch 39 ; accuracy: 0.8366666666666667; loss: 0.6625950336456299\n",
      "Training epoch 40 ; accuracy: 1.0; loss: 0.021084776148200035\n",
      "Validation epoch 40 ; accuracy: 0.8333333333333334; loss: 0.6669073104858398\n",
      "Training epoch 41 ; accuracy: 1.0; loss: 0.017979979515075684\n",
      "Validation epoch 41 ; accuracy: 0.8333333333333334; loss: 0.6713517904281616\n",
      "Training epoch 42 ; accuracy: 1.0; loss: 0.018449634313583374\n",
      "Validation epoch 42 ; accuracy: 0.8366666666666667; loss: 0.6758060455322266\n",
      "Training epoch 43 ; accuracy: 1.0; loss: 0.01795525848865509\n",
      "Validation epoch 43 ; accuracy: 0.8366666666666667; loss: 0.6810486316680908\n",
      "Training epoch 44 ; accuracy: 1.0; loss: 0.01451125182211399\n",
      "Validation epoch 44 ; accuracy: 0.8366666666666667; loss: 0.6861230731010437\n",
      "Training epoch 45 ; accuracy: 1.0; loss: 0.013751566410064697\n",
      "Validation epoch 45 ; accuracy: 0.8366666666666667; loss: 0.6908122897148132\n",
      "Training epoch 46 ; accuracy: 1.0; loss: 0.012980515137314796\n",
      "Validation epoch 46 ; accuracy: 0.8366666666666667; loss: 0.6954931616783142\n",
      "Training epoch 47 ; accuracy: 1.0; loss: 0.012399105355143547\n",
      "Validation epoch 47 ; accuracy: 0.8366666666666667; loss: 0.7000882625579834\n",
      "Training epoch 48 ; accuracy: 1.0; loss: 0.011885504238307476\n",
      "Validation epoch 48 ; accuracy: 0.8366666666666667; loss: 0.7044748067855835\n",
      "Training epoch 49 ; accuracy: 1.0; loss: 0.010726498439908028\n",
      "Validation epoch 49 ; accuracy: 0.8366666666666667; loss: 0.7086379528045654\n",
      "Training epoch 50 ; accuracy: 1.0; loss: 0.010401477105915546\n",
      "Validation epoch 50 ; accuracy: 0.8366666666666667; loss: 0.7126645445823669\n",
      "Training epoch 51 ; accuracy: 1.0; loss: 0.009272103197872639\n",
      "Validation epoch 51 ; accuracy: 0.8333333333333334; loss: 0.716248095035553\n",
      "Training epoch 52 ; accuracy: 1.0; loss: 0.009458324871957302\n",
      "Validation epoch 52 ; accuracy: 0.8333333333333334; loss: 0.7200977802276611\n",
      "Training epoch 53 ; accuracy: 1.0; loss: 0.0086722606793046\n",
      "Validation epoch 53 ; accuracy: 0.8333333333333334; loss: 0.7239060997962952\n",
      "Training epoch 54 ; accuracy: 1.0; loss: 0.008034810423851013\n",
      "Validation epoch 54 ; accuracy: 0.83; loss: 0.7275111675262451\n",
      "Training epoch 55 ; accuracy: 1.0; loss: 0.007648202124983072\n",
      "Validation epoch 55 ; accuracy: 0.83; loss: 0.7310800552368164\n",
      "Training epoch 56 ; accuracy: 1.0; loss: 0.007005623541772366\n",
      "Validation epoch 56 ; accuracy: 0.8266666666666667; loss: 0.7346957325935364\n",
      "Training epoch 57 ; accuracy: 1.0; loss: 0.0073982723988592625\n",
      "Validation epoch 57 ; accuracy: 0.8266666666666667; loss: 0.7376213073730469\n",
      "Training epoch 58 ; accuracy: 1.0; loss: 0.006323896814137697\n",
      "Validation epoch 58 ; accuracy: 0.8266666666666667; loss: 0.7402118444442749\n",
      "Training epoch 59 ; accuracy: 1.0; loss: 0.006095294374972582\n",
      "Validation epoch 59 ; accuracy: 0.8266666666666667; loss: 0.7426930069923401\n",
      "Training epoch 60 ; accuracy: 1.0; loss: 0.006229606457054615\n",
      "Validation epoch 60 ; accuracy: 0.8266666666666667; loss: 0.7450043559074402\n",
      "Training epoch 61 ; accuracy: 1.0; loss: 0.006287954747676849\n",
      "Validation epoch 61 ; accuracy: 0.8266666666666667; loss: 0.7472513914108276\n",
      "Training epoch 62 ; accuracy: 1.0; loss: 0.005399817600846291\n",
      "Validation epoch 62 ; accuracy: 0.8266666666666667; loss: 0.7494724988937378\n",
      "Training epoch 63 ; accuracy: 1.0; loss: 0.0054400949738919735\n",
      "Validation epoch 63 ; accuracy: 0.8266666666666667; loss: 0.7517129778862\n",
      "Training epoch 64 ; accuracy: 1.0; loss: 0.005213822703808546\n",
      "Validation epoch 64 ; accuracy: 0.8266666666666667; loss: 0.754123866558075\n",
      "Training epoch 65 ; accuracy: 1.0; loss: 0.005017415154725313\n",
      "Validation epoch 65 ; accuracy: 0.8266666666666667; loss: 0.756527841091156\n",
      "Training epoch 66 ; accuracy: 1.0; loss: 0.005029722582548857\n",
      "Validation epoch 66 ; accuracy: 0.8266666666666667; loss: 0.7589121460914612\n",
      "Training epoch 67 ; accuracy: 1.0; loss: 0.0048861149698495865\n",
      "Validation epoch 67 ; accuracy: 0.8266666666666667; loss: 0.7610170245170593\n",
      "Training epoch 68 ; accuracy: 1.0; loss: 0.004889471922069788\n",
      "Validation epoch 68 ; accuracy: 0.8266666666666667; loss: 0.7632458209991455\n",
      "Training epoch 69 ; accuracy: 1.0; loss: 0.004579133354127407\n",
      "Validation epoch 69 ; accuracy: 0.8266666666666667; loss: 0.7654106616973877\n",
      "Training epoch 70 ; accuracy: 1.0; loss: 0.004545805510133505\n",
      "Validation epoch 70 ; accuracy: 0.8266666666666667; loss: 0.7674176692962646\n",
      "Training epoch 71 ; accuracy: 1.0; loss: 0.004353018943220377\n",
      "Validation epoch 71 ; accuracy: 0.8266666666666667; loss: 0.7693279981613159\n",
      "Training epoch 72 ; accuracy: 1.0; loss: 0.00403566425666213\n",
      "Validation epoch 72 ; accuracy: 0.8233333333333334; loss: 0.7711485028266907\n",
      "Training epoch 73 ; accuracy: 1.0; loss: 0.004379931837320328\n",
      "Validation epoch 73 ; accuracy: 0.8233333333333334; loss: 0.772674024105072\n",
      "Training epoch 74 ; accuracy: 1.0; loss: 0.004334511701017618\n",
      "Validation epoch 74 ; accuracy: 0.8266666666666667; loss: 0.773838996887207\n",
      "Training epoch 75 ; accuracy: 1.0; loss: 0.0036609861999750137\n",
      "Validation epoch 75 ; accuracy: 0.8266666666666667; loss: 0.7749736309051514\n",
      "Training epoch 76 ; accuracy: 1.0; loss: 0.0035366928204894066\n",
      "Validation epoch 76 ; accuracy: 0.8266666666666667; loss: 0.776070773601532\n",
      "Training epoch 77 ; accuracy: 1.0; loss: 0.003634346416220069\n",
      "Validation epoch 77 ; accuracy: 0.8266666666666667; loss: 0.7772156000137329\n",
      "Training epoch 78 ; accuracy: 1.0; loss: 0.003786996006965637\n",
      "Validation epoch 78 ; accuracy: 0.8266666666666667; loss: 0.7784345149993896\n",
      "Training epoch 79 ; accuracy: 1.0; loss: 0.0037941820919513702\n",
      "Validation epoch 79 ; accuracy: 0.8266666666666667; loss: 0.779690146446228\n",
      "Training epoch 80 ; accuracy: 1.0; loss: 0.003610539948567748\n",
      "Validation epoch 80 ; accuracy: 0.8266666666666667; loss: 0.7806511521339417\n",
      "Training epoch 81 ; accuracy: 1.0; loss: 0.003463550005108118\n",
      "Validation epoch 81 ; accuracy: 0.8266666666666667; loss: 0.7815885543823242\n",
      "Training epoch 82 ; accuracy: 1.0; loss: 0.0033873405773192644\n",
      "Validation epoch 82 ; accuracy: 0.8266666666666667; loss: 0.782605767250061\n",
      "Training epoch 83 ; accuracy: 1.0; loss: 0.0036590907257050276\n",
      "Validation epoch 83 ; accuracy: 0.8266666666666667; loss: 0.7833156585693359\n",
      "Training epoch 84 ; accuracy: 1.0; loss: 0.002978365868330002\n",
      "Validation epoch 84 ; accuracy: 0.8266666666666667; loss: 0.7841358184814453\n",
      "Training epoch 85 ; accuracy: 1.0; loss: 0.003472590586170554\n",
      "Validation epoch 85 ; accuracy: 0.8266666666666667; loss: 0.7853151559829712\n",
      "Training epoch 86 ; accuracy: 1.0; loss: 0.0032221649307757616\n",
      "Validation epoch 86 ; accuracy: 0.8266666666666667; loss: 0.7864671945571899\n",
      "Training epoch 87 ; accuracy: 1.0; loss: 0.0030720962677150965\n",
      "Validation epoch 87 ; accuracy: 0.8266666666666667; loss: 0.7876203060150146\n",
      "Training epoch 88 ; accuracy: 1.0; loss: 0.002922150306403637\n",
      "Validation epoch 88 ; accuracy: 0.8266666666666667; loss: 0.7886397242546082\n",
      "Training epoch 89 ; accuracy: 1.0; loss: 0.0031403889879584312\n",
      "Validation epoch 89 ; accuracy: 0.8266666666666667; loss: 0.7898721694946289\n",
      "Training epoch 90 ; accuracy: 1.0; loss: 0.003178381361067295\n",
      "Validation epoch 90 ; accuracy: 0.8266666666666667; loss: 0.7909478545188904\n",
      "Training epoch 91 ; accuracy: 1.0; loss: 0.0029614681843668222\n",
      "Validation epoch 91 ; accuracy: 0.8266666666666667; loss: 0.792057991027832\n",
      "Training epoch 92 ; accuracy: 1.0; loss: 0.0026494874618947506\n",
      "Validation epoch 92 ; accuracy: 0.8266666666666667; loss: 0.7931197881698608\n",
      "Training epoch 93 ; accuracy: 1.0; loss: 0.002957410179078579\n",
      "Validation epoch 93 ; accuracy: 0.8266666666666667; loss: 0.7944188714027405\n",
      "Training epoch 94 ; accuracy: 1.0; loss: 0.0026842288207262754\n",
      "Validation epoch 94 ; accuracy: 0.8266666666666667; loss: 0.795785129070282\n",
      "Training epoch 95 ; accuracy: 1.0; loss: 0.002771292347460985\n",
      "Validation epoch 95 ; accuracy: 0.8266666666666667; loss: 0.7973648309707642\n",
      "Training epoch 96 ; accuracy: 1.0; loss: 0.002509581157937646\n",
      "Validation epoch 96 ; accuracy: 0.8266666666666667; loss: 0.7987813353538513\n",
      "Training epoch 97 ; accuracy: 1.0; loss: 0.0027367069851607084\n",
      "Validation epoch 97 ; accuracy: 0.8266666666666667; loss: 0.8001395463943481\n",
      "Training epoch 98 ; accuracy: 1.0; loss: 0.002570078708231449\n",
      "Validation epoch 98 ; accuracy: 0.8266666666666667; loss: 0.8015142679214478\n",
      "Training epoch 99 ; accuracy: 1.0; loss: 0.0023175878450274467\n",
      "Validation epoch 99 ; accuracy: 0.8266666666666667; loss: 0.8027909994125366\n"
     ]
    }
   ],
   "source": [
    "epochs=100\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.01)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_labels=labels[idx_train]\n",
    "    val_labels=labels[idx_val]\n",
    "    \n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss=F.nll_loss(output[idx_train],train_labels)\n",
    "    print(f\"Training epoch {epoch} ; accuracy: {accuracy(output[idx_train],train_labels)}; loss: {loss.item()}\")\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss=F.nll_loss(output[idx_val],val_labels)\n",
    "    print(f\"Validation epoch {epoch} ; accuracy: {accuracy(output[idx_val],val_labels)}; loss: {loss.item()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set ; accuracy: 0.8; loss: 0.6505059599876404\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_labels=labels[idx_test]\n",
    "output = model(features, adj)\n",
    "loss=F.nll_loss(output[idx_test],test_labels)\n",
    "print(f\"Test set ; accuracy: {accuracy(output[idx_test],test_labels)}; loss: {loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
